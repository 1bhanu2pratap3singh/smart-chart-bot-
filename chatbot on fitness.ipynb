{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48598c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f56bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import openai\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Define the file names\n",
    "file_names = [\n",
    "    \"Nutrition Guide for Physicians (Nutrition and Health) ( PDFDrive ).pdf\",\n",
    "    \"Genius Foods- Become Smarter, Happier, and More Productive While Protecting Your Brain for Life ( PDFDrive ).pdf\",\n",
    "    \"Nutrition Book, How to Gain Muscle, Weight Training, How to Lose Weight, Diet book, Protein Diet Optimal Guide To Your Best Physique- Raw Truth Behind Nutrition & Training ( PDFDrive ).pdf\",\n",
    "    \"Encyclopedia of Foods. A Guide to Healthy Nutrition ( PDFDrive ).pdf\",\n",
    "    \"Complete Food and Nutrition Guide ( PDFDrive ).pdf\",\n",
    "    \"Encyclopedia of Bodybuilding- The Complete A-Z Book on Muscle Building ( PDFDrive ).pdf\",\n",
    "    \"Encyclopedia of Diets - A Guide to Health and Nutrition ( PDFDrive ).pdf\",\n",
    "    \"The New Encyclopedia of Modern Bodybuilding - The Bible of Bodybuilding, Fully Updated and Revised ( PDFDrive ).pdf\",\n",
    "    \"Beyond-Brawn-2nd-Edition.pdf\",\n",
    "    \"101 Workouts- Build Muscle, Lose Fat & Reach Your Fitness Goals Faster ( PDFDrive ).pdf\",\n",
    "    \"The Total Fitness Manual- Transform Your Body in Just 12 Weeks ( PDFDrive ).pdf\"\n",
    "]\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    cleaned_text = re.sub(r'\\W+', ' ', text).lower()\n",
    "\n",
    "    # Tokenize the text and remove stop words\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Define a function to split the text into passages\n",
    "def split_text(text, chunk_size):\n",
    "    # Split the text into chunks of a fixed size\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Define the chunk size for splitting the text\n",
    "chunk_size = 500\n",
    "\n",
    "# Load the contents of the PDF files\n",
    "contents = []\n",
    "for i, file_name in enumerate(file_names):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text = ''\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            page_text = page.extract_text()\n",
    "            text += page_text\n",
    "        contents.append(text)\n",
    "# Preprocess the text\n",
    "processed_contents = [preprocess_text(content) for content in contents]\n",
    "# Split the text into passages\n",
    "passages = []\n",
    "for i, content in enumerate(processed_contents):\n",
    "    chunks = split_text(content, chunk_size)\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        passage = {\n",
    "            \"id\": f\"{i}_{j}\",\n",
    "            \"text\": chunk,\n",
    "            \"document\": f\"{file_names[i]}\"\n",
    "        }\n",
    "        passages.append(passage)\n",
    "        question = \"What is a healthy diet?\"\n",
    "passage_texts = [passage['text'] for passage in passages]\n",
    "\n",
    "# Set up the OpenAI API request\n",
    "model_engine = \"text-davinci-002\"\n",
    "prompt = f\"Answer the following question:\\n\\n{question}\\n\\nPassages:\\n\"\n",
    "for i, passage_text in enumerate(passage_texts):\n",
    "    prompt += f\"{i+1}. {passage_text}\\n\\n\"\n",
    "\n",
    "# Set the maximum prompt length\n",
    "max_prompt_length = 2000\n",
    "\n",
    "# Send the request to the OpenAI API for each chunk of the prompt\n",
    "response_chunks = []\n",
    "for i in range(0, len(prompt), max_prompt_length):\n",
    "    prompt_chunk = prompt[i:i+max_prompt_length]\n",
    "    response = openai.Completion.create(\n",
    "        engine=model_engine,\n",
    "        prompt=prompt_chunk,\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "    response_chunks.append(response.choices[0].text.strip())\n",
    "\n",
    "# Combine the responses into a single string\n",
    "answer = ' '.join(response_chunks)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35353be",
   "metadata": {},
   "source": [
    "The required packages and libraries are imported at the beginning of the code.\n",
    "The OpenAI API key is set up to access the API.\n",
    "A list of PDF files is defined, each containing information related to nutrition and fitness.\n",
    "A function preprocess_text() is defined to preprocess the text from each PDF file to remove non-alphanumeric characters, convert to lowercase, tokenize, remove stop words, and lemmatize the tokens.\n",
    "Another function split_text() is defined to split the preprocessed text into chunks of a fixed size (here, 500 words per chunk).\n",
    "The contents of each PDF file are loaded and preprocessed using the preprocess_text() function.\n",
    "The preprocessed contents are then split into passages using the split_text() function, and each passage is given an ID, text, and document name.\n",
    "A question is defined (\"What is a healthy diet?\") to which the API will provide an answer based on the given passages.\n",
    "The passages are converted into a formatted prompt string that includes the question and the text of each passage.\n",
    "The prompt string is sent to the OpenAI API using the text-davinci-002 model, which is a natural language processing model that can generate human-like responses to text prompts.\n",
    "The API returns a response in chunks, and each chunk is combined into a single string that represents the answer to the question.\n",
    "Finally, the answer is printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66707c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
